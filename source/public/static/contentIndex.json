{"News/2025/02/PhD-position-available-at-the-lab":{"title":"PhD position available at the lab","links":[],"tags":["DIY","creative-AI","nature-inspired-computing"],"content":"\nImage generated using Craiyon AI\nWe are happy to announce an exciting PhD position to work on “Nature-inspired computing for sound-based DIY approaches to creative AI” at the Centre for Digital Music, School of Electronic Engineering and Computer Science, Queen Mary University of London.\n\nApplication deadline: 28th February 2025\nRequirements: UK home student\nHow to apply: www.qmul.ac.uk/eecs/phd/how-to-apply/\nYou can find more info here: www.findaphd.com/phds/project/nature-inspired-computing-for-sound-based-diy-approaches-to-creative-ai/\n"},"News/2025/02/index":{"title":"02","links":[],"tags":[],"content":""},"News/2025/05/BSc-student-Stanley-Parker-wins-a-hackathon":{"title":"BSc student Stanley Parker wins a hackathon","links":[],"tags":[],"content":"\nBlueCrabs team at the Open Sea Lab 4.0 Hackathon\nCrab Alert: Hackathon team builds award-winning early warning system for invasive species.\nStanley Parker, a third-year BSc Creative Computing student at the School of Electronic Engineering and Computer Science, recently took part in Open Sea Lab 4.0, a prestigious international hackathon focused on ocean innovation."},"News/2025/05/Dr-Anna-Xambó---keynote,-paper-and-performance-at-ICLC2025":{"title":"Dr Anna Xambó - keynote, paper and performance at ICLC2025","links":[],"tags":["presentations","live-coding","liveness","conferences"],"content":"\nICLC 2025 logo\nDr Anna Xambó will be giving a keynote on Wednesday 28 May 16:30-17:30 on “Liveness as an open work: an ongoing live-coding algorithmic journey” at the International Conference on Live Coding (ICLC).\nOn Thursday 29 May she will also deliver the paper co-authored paper “Building a Dataset of Personal Live Coding Style Using MIRLCaProxy - A Journal of Creative Sonic Exploration under Constraints and Biases” and will perform in the evening the live-coding session “Sensing the Alice Holt Forest”.\nFor more info visit: iclc.toplap.org/2025/"},"News/2025/05/index":{"title":"05","links":[],"tags":[],"content":""},"News/2025/06/index":{"title":"06","links":[],"tags":[],"content":""},"News/2025/09/Dr-Gerard-Roma-and-Dr-Anna-Xambó-present-a-poster-at-UKAIRS-2025":{"title":"Dr Gerard Roma and Dr Anna Xambó present a poster at UKAIRS 2025","links":[],"tags":["UKRI","sensing-the-forest","UKAIRS","conferences","soundscape-based-music","creative-AI"],"content":"\nFrom left to right: Anna Xambó and Gerard Roma.\nThe inaugural UK AI Research Symposium (UKAIRS) took place at Northumbria University, 8-9 September 2025, in Newcastle upon Tyne, UK – the first event to bring together researchers from a variety of academic disciplines, all working within the AI community.\nGerard Roma and Anna Xamb´ø presented the Sensing the Forest project on Tuesday, 9th September 2025, with the poster:\n\nXambó, Anna; Batchelor, Peter; Marino, Luigi; Roma, Gerard; Bell, Mike; and Xenakis, George (2025). Soundscape-based music and creative AI: Insights and promises, UK AI Research Symposium (UKAIRS) 2025, Newcastle, UK.\n\n\nAbstract: Technological revolution and industrialisation are, sadly, disconnecting us, humans, from our natural environment. This loss of connection with nature is waning activities such as listening to natural sounds. Monitoring and understanding the natural sounds of our environment can help identify possible changes or anomalies, which in turn can inform the bigger picture of depletion of natural resources, loss of biodiversity, and climate change, among others. This position paper investigates the insights and promises of creative uses of AI applied to soundscape-based music in the form of musical instruments and practices. By looking at AI-enhanced bespoke DIY technologies for streaming and analysing live soundscapes, live coding with natural sounds or processing sound events, we can shed light on generating and manipulating new sonic material for music performance. This approach can raise awareness of the fundamental connection between sound and the environment.\n"},"News/2025/09/index":{"title":"09","links":[],"tags":[],"content":""},"News/2025/10/Dr-Anna-Xambó-presents-at-the-Freesound-Day-programme":{"title":"Dr Anna Xambó presents at the Freesound Day programme","links":[],"tags":["climate-change","sensing-the-forest","freesound","presentations"],"content":"\nDr Anna Xambó will be presenting “Sensing the Forest: Exploring Climate Change Through Soundscape Datasets from DIY Streamers at Alice Holt Forest” at the Freesound Day programme, October 28th, Barcelona and online\nThe programme includes a full day of talks. If you are attending remotely, all you need to do is to join this Zoom meeting room (the room will be open 15 minutes before the start of the event)"},"News/2025/10/Qiaoxi-Zhang-presents-at-TENOR-2025":{"title":"Qiaoxi Zhang  presents at TENOR 2025","links":[],"tags":["conferences","AI-music"],"content":"\nQiaoxi Zhang will be presenting the following paper at the TENOR 2025 conference (October 25-27, 2025):\n\nZhang, Q., Barthet, M., Xambó Sedó, A. (accepted) “From Shape to Music: Contour-Conditioned Symbolic Music Generation”. Proceedings of the International Conference on Technologies for Music Notation and Representation (TENOR 2025). Central Conservatory of Music Beijing, Beijing, China.\n\n\nAbstract: We present a novel approach to symbolic music generation that enables users to guide melodic trajectories through abstract contour inputs. This work demonstrates how low-effort, sketch-based controls can influence the expressive shape of generated music, paving the way for more intuitive composition tools. Our framework consists of two main stages. In the first stage, we extract melodic contour from symbolic music using the Ramer–Douglas–Peucker (RDP) algorithm, which simplifies the melodic trajectory while preserving its essential directional shape. Second, a Transformer-based generative model is conditioned on user-specified contour tokens to produce polyphonic continuations that follow the desired melodic shape. To strengthen contour adherence, we introduce a contour-aware loss based on cosine similarity between the target contour vector and the generated melodic motion.\nObjective evaluations on the MAESTRO dataset indicate that the proposed model closely aligns with target contours while preserving overall musical quality comparable to a baseline Music Transformer. These results highlight the feasibility of high-level contour-conditioned music generation and point toward future applications that integrate contour-based inputs with modalities such as hand gestures in VR, enabling intuitive, real-time human–AI co-composition.\n\nThe International Conference on Technologies for Music Notation and Representation is dedicated to issues in theoretical and applied research and development in Music Notation and Representation, with a strong focus on computer tools and applications, as well as a tight connection to music creation. “Technology” in the conference name refers to any mean that may contribute to the notation, representation and/or visualisation of the music and sound, for purposes that may include (but not limited to) music composition, performance, representation, transcription, analysis and pedagogy."},"News/2025/10/index":{"title":"10","links":[],"tags":[],"content":""},"News/2025/11/index":{"title":"11","links":[],"tags":[],"content":""},"News/2025/index":{"title":"2025","links":[],"tags":[],"content":""},"News/index":{"title":"News","links":[],"tags":[],"content":""},"about":{"title":"About","links":[],"tags":[],"content":"\nThe lab aims to become a research hub in developing sustainable, inclusive, and forward-thinking technologies that transform how we create, experience, and understand sound and music computing.\n\nThe Computational Sonic Arts Laboratory (CSAL) is a research team based in the Centre for Digital Music (C4DM) at Queen Mary University of London dedicated to advancing the intersection of sonic arts and cutting-edge technology. The lab is led by Dr Anna Xambó Sedó and has been founded in 2025 as part of QMUL’s Centre for Digital Music.\nRooted in principles of culture, creativity, and community, the lab explores sonic creativities and creative computing through innovative research in creative AI, music AI, and intelligent music systems. The vision of the lab is to bridge HCI, sound and music computing, and new interfaces for musical expression, by emphasising live coding, network music, and generative sound-based music. The lab aims to become a research hub in developing sustainable, inclusive, and forward-thinking technologies that transform how we create, experience, and understand music.\nResearch activities include:\n\nThe design, deployment and evaluation of intelligent sound-based music systems that keep the human in the loop and give ownership to communities of practice and perspectives generally underrepresented in music AI.\nThe creation of sound-based music performances and sonic arts experiences that foster democratic principles in music making and raise awareness of real-world problems.\nThe design and development of sustainable and DIY systems based on interdisciplinary methods involving art, science and engineering and in alignment with open source, open hardware, citizen science and the Maker’s Bill of Rights.\n\nThe lab hosts the AHRC-funded project Sensing the Forest - Let the Forest Speak using the Internet of Things, Acoustic Ecology and Creative AI, which pursues raising awareness and understanding of forest environmental data and how they relate to climate change.\nPlease, get in touch if you are interested in PhD opportunities. The Centre for Digital Music of Queen Mary University of London welcomes PhD applications for 2025."},"index":{"title":"Welcome to the Computational Sonic Arts Lab | C4DM | QMUL |","links":["about","publications","people","News/2025/10/Dr-Anna-Xambó-presents-at-the-Freesound-Day-programme","News/2025/10/Qiaoxi-Zhang-presents-at-TENOR-2025","News/2025/09/Dr-Gerard-Roma-and-Dr-Anna-Xambó-present-a-poster-at-UKAIRS-2025"],"tags":[],"content":"about | publications | people\n\nFrom left to right: Shuoyang Zheng, Lianganzi Wang, Anna Xambó Sedó, Nico García-Peguinho, Merlin Goldman and Jimena Arruti. Top, from left to right: Lina Bautista, Qiaoxi Zhang and Solomiya Moroz. Photo and photo composition by Shuoyang Zheng.\nLatest News\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDr Anna Xambó presents at the Freesound Day programme Anna will be presenting “Sensing the Forest: Exploring Climate Change Through Soundscape Datasets from DIY Streamers at Alice Holt Forest” at the Freesound Day programme, October 28th, Barcelona and onlineQiaoxi Zhang presents at TENOR 2025 Qiaoxi will be presenting the following paper at the TENOR 2025 conference (October 25-27, 2025): Zhang, Q., Barthet, M., Xambó Sedó, A. “From Shape to Music: Contour-Conditioned Symbolic Music Generation”.Dr Gerard Roma and Dr Anna Xambó present a poster at UKAIRS 2025 Gerard and Anna presented the Sensing the Forest project with the poster: Xambó, Anna; Batchelor, Peter; Marino, Luigi; Roma, Gerard; Bell, Mike; and Xenakis, George (2025). Soundscape-based music and creative AI: Insights and promises, UK AI Research Symposium (UKAIRS) 2025, Newcastle, UK."},"people":{"title":"People","links":[],"tags":[],"content":"Members\n\nDr Anna Xambó (Senior Lecturer in Sound and Music Computing, Queen Mary University of London)\nShuoyang Zheng (PhD student 2023-present, co-supervision with Nick Bryan-Kinns, AIM/C4DM, Queen Mary University of London)\nQiaoxi Zhang (PhD student 2023-present, co-supervision with Mathieu Barthet, AIM/C4DM, Queen Mary University of London)\nNicolás António García-Peguinho (PhD student 2025-present, co-supervision with Fabrizio Smeraldi, C4DM, Queen Mary University of London)\nLianganzi Wang (PhD student 2025-present, C4DM, co-supervision with Iran Roman)\nLina Bautista (PhD student 2025-present, co-supervision with Enric Mor, UOC)\nDr Luigi Marino (Research Fellow in Sound and Music Computing, Queen Mary University of London, 2023-2025)\nTug O’Flaherty (MSc Sound and Music Computing, Queen Mary University of London, MSc project 2024-25)\nMerlin Goldman (PhD student starting in 2026, C4DM, co-supervision with Mark Sandler)\n\nUG project students\n\nAleena Nizami (BSc Computer Science and AI, Queen Mary University of London, BSc project 2025-2026)\nDharma Marmalade Rumpal (BSc Computer Science. Queen Mary University of London, BSc project 2025-2026)\nHaidar Chawki Kassem (BSc Computer Science and AI, Queen Mary University of London, BSc project 2025-2026)\nHudhayfa Abdus-Salaam Ahmed (BSc Computer Science, Queen Mary University of London, BSc project 2025-2026)\nMolly Hall (BSc Computer Science,  Queen Mary University of London, BSc project 2025-2026)\nVeer Arora (BSc Computer Science,  Queen Mary University of London, BSc project 2025-2026)\n\nAlumni\nMSc project students\n\nXinyue Xu (MSc Sound and Music Computing, Queen Mary University of London, MSc project 2024-2025)\nAndrés Sánchez Castrillón (MSc Artificial Intelligence, Queen Mary University of London, MSc project 2024-2025)\nJames Shortland (MSc Data Science and Artificial Intelligence, Queen Mary University of London, MSc project 2024-2025)\n\nUG project students\n\nAleksander Skutnik (BSc Computer Science, Queen Mary University of London, BSc project 2024-2025)\nStanley Parker (BSc Creative Computing, Queen Mary University of London, BSc project 2024-2025)\nNing Liu (BSc(Eng)FT Electronic Engineering, Queen Mary University of London, BSc project 2024-2025)\nAmrina Kaur Virk (BSc Creative Computing, Queen Mary University of London, BSc project 2024-2025)\n"},"publications":{"title":"Publications","links":[],"tags":[],"content":"2025\n\nZheng, S. J., Xambó Sedó, A., &amp; Bryan-Kinns, N. (2025). Exploring gestural affordances in audio latent space navigation. Frontiers in Computer Science. doi.org/10.3389/fcomp.2025.1575202\nZhang, Q., Barthet, M., Xambó Sedó, A. (accepted) “From Shape to Music: Contour-Conditioned Symbolic Music Generation”. Proceedings of the International Conference on Technologies for Music Notation and Representation (TENOR 2025). Central Conservatory of Music Beijing, Beijing, China.\nO’Flaherty, T. F., Elmokadem, M. B., Xu, X., Manjunatha, K. N., Roma, G., Xenakis, G., Xambó Sedó, A., (accepted) “Sonification Mappings for Sensing Tree Stress: A DIY Approach”. Proceedings of the Web Audio Conference 2025 (WAC 2025). Ircam/Mozilla, Paris, France.\nXambó Sedó, Anna; Batchelor, Peter; Marino, Luigi; Roma, Gerard; Bell, Mike; and Xenakis, George (2025). Soundscape-based music and creative AI: Insights and promises, UK AI Research Symposium (UKAIRS) 2025, Newcastle, UK.\nXambó Sedó, A., Roma, G. (2025) Building a Dataset of Personal Live Coding Style Using MIRLCaProxy - A Journal of Creative Sonic Exploration under Constraints and Biases. In Proceedings of the International Conference of Live Coding. doi.org/10.5281/zenodo.15698928\nO’Flaherty, T. F., Marino, L., Saitis, C., Xambó Sedó, A. (2025) Sonicolour: Exploring Colour Control of Sound Synthesis with Interactive Machine Learning. In Proceedings of the New Interfaces for Musical Expression.\nXambó Sedó, A. (2025) Live Coding a Chorale of Sounds Using MIRLCa: State of Affairs and Implications. SuperCollider Symposium 2025, Johns Hopkins University Bloomberg Center, Washington D.C., USA. doi.org/10.5281/zenodo.15283062\n\n2024\n\nXambó Sedó, A., Roma, G. (2024) Human–machine agencies in live coding for music performance. Journal of New Music Research, 53(1–2), 33–46. doi.org/10.1080/09298215.2024.2442355.\nMarino, L., Xambó Sedó, A. (2024) Developing DIY solar-powered, off-grid audio streamers for forest soundscapes: progress and challenges. Proceedings of CHIME Annual Conference, The Open University, 1-2 December 2024.\nZheng, S., Xambó Sedó, A., Bryan-Kinns, N. (2024) “A Mapping Strategy for Interacting with Latent Audio Synthesis Using Artistic Materials”. 2nd international workshop on eXplainable AI for the Arts (XAIxArts) at the ACM Creativity and Cognition Conference.\nZheng, S., Del Sette, B.M., Saitis, C., Xambó Sedó, A., Bryan-Kinns, N. (2024) “Building Sketch-to-Sound Mapping with Unsupervised Feature Extraction and Interactive Machine Learning”. In Proceedings of the New Interfaces for Musical Expression (NIME 24). Utrecht, The Netherlands.\n"}}